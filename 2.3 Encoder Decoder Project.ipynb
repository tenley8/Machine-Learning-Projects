{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.3 Encoder Decoder Project.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMD3NGABuEsl/UwaDcVjNU6"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xT2VI3Q7ojsf"},"source":["## **Encoder-Decoder Project:** \n"]},{"cell_type":"markdown","metadata":{"id":"h2_Jq-C2m5G6"},"source":["## **Context**\n","The Encoder-Decoder architecture with recurrent neural networks has become an effective and standard approach for both neural machine translation (NMT) and **sequence-to-sequence (seq2seq)** prediction in general.\n","\n","The key benefits of the approach are the ability to train a single end-to-end model directly on source and target sentences and the ability to handle variable length input and output sequences of text.\n","## **Content**\n","Train an Encoder–Decoder model that can convert a date string from one format to another (e.g., from \"April 22, 2019\" to \"2019-04-22\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RnhWG5VMpu31"},"source":["### **Loading data and preparing dataset**\n","- import libraries\n","- create dataset"]},{"cell_type":"code","metadata":{"id":"fltrxTgMp2OH"},"source":["# import libraries\n","import os\n","import sys\n","import numpy as np\n","import sklearn\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","\n","# plot figures\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fgCyQxG8FzWJ"},"source":["### **Preprocess data**\n","- Create dataset dates: 1000-01-01 and 9999-12-31\n","- Print random dates with input and output target format\n","- INPUT_CHARS: Get list of character inputs\n","- OUTPUT_CHARS: Show list of character outputs\n","- Create function to convert string to character IDs list"]},{"cell_type":"markdown","metadata":{"id":"DzuRhqkhogeP"},"source":["Let's start by creating the dataset. We will use random days between 1000-01-01 and 9999-12-31:"]},{"cell_type":"code","metadata":{"id":"aALC_SSgm4SB"},"source":["# create dataset\n","from datetime import date\n","\n","# cannot use strftime()'s %B format since it depends on the locale\n","MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n","          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n","\n","def random_dates(n_dates):\n","    min_date = date(1000, 1, 1).toordinal()\n","    max_date = date(9999, 12, 31).toordinal()\n","\n","    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n","    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n","\n","    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n","    y = [dt.isoformat() for dt in dates]\n","    return x, y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U40xrjo8FQ4s"},"source":["Here are a few random dates, displayed in both the input format and the target format:"]},{"cell_type":"code","metadata":{"id":"RyK3TFo3FUVT","colab":{"base_uri":"https://localhost:8080/","height":111},"executionInfo":{"status":"ok","timestamp":1603450276123,"user_tz":240,"elapsed":476,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"47e79082-a7e6-4b45-f551-0d4710e8ba94"},"source":["# random dates\n","np.random.seed(42)\n","\n","n_dates = 3\n","x_example, y_example = random_dates(n_dates)\n","print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n","print(\"-\" * 50)\n","for idx in range(n_dates):\n","    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input                    Target                   \n","--------------------------------------------------\n","September 20, 7075       7075-09-20               \n","May 15, 8579             8579-05-15               \n","January 11, 7103         7103-01-11               \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DrkYlA9KqjFd"},"source":["Let's get the list of all possible characters in the inputs:"]},{"cell_type":"code","metadata":{"id":"Bw4IRT7tqkRT","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1603450200859,"user_tz":240,"elapsed":248,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"3ab7af51-08de-4409-e1bd-88cb463363d5"},"source":["# char input list\n","INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS)))) + \"01234567890, \"\n","INPUT_CHARS"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'ADFJMNOSabceghilmnoprstuvy01234567890, '"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"aalVFHLjqr5b"},"source":["And here's the list of possible characters in the outputs:"]},{"cell_type":"code","metadata":{"id":"56rFLBzcINmX"},"source":["# char outputs\n","OUTPUT_CHARS = \"0123456789-\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Bo9WaWZIZ1e"},"source":["Let's write a function to convert a string to a list of character IDs\n","- date_str_to for input char and output chars\n","- function prepare_date_str\n","- function create_dataset"]},{"cell_type":"code","metadata":{"id":"0CneYaRNIg3d"},"source":["def date_str_to_ids(date_str, chars=INPUT_CHARS):\n","    return [chars.index(c) for c in date_str]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPhqr2SXIilI","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1603451114777,"user_tz":240,"elapsed":391,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"ee6cc831-d159-4a9a-b2e2-dd48ad4c84b1"},"source":["date_str_to_ids(x_example[0], INPUT_CHARS)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[7, 11, 19, 22, 11, 16, 9, 11, 20, 38, 28, 26, 37, 38, 33, 26, 33, 31]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"KBjssGadImeV","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1603451192090,"user_tz":240,"elapsed":379,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"729e039e-2523-4242-f413-68b06182c48f"},"source":["date_str_to_ids(y_example[0], OUTPUT_CHARS)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[7, 0, 7, 5, 10, 0, 9, 10, 2, 0]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"-GpDxpG_I4om"},"source":["def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n","    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n","    X = tf.ragged.constant(X_ids, ragged_rank=1)\n","    return (X + 1).to_tensor() # using 0 as the padding token ID\n","\n","def create_dataset(n_dates):\n","    x, y = random_dates(n_dates)\n","    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XD-Sg9--JTa3"},"source":["### **Create training set**\n","- train dataset: 10000\n","- valid dataset: 2000\n","- test dataset: 2000\n"]},{"cell_type":"code","metadata":{"id":"XbKjZxJFJ1rV"},"source":["# train, validate, test\n","np.random.seed(42)\n","\n","X_train, Y_train = create_dataset(10000)\n","X_valid, Y_valid = create_dataset(2000)\n","X_test, Y_test = create_dataset(2000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YhETtTXmJ-Y2","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1603451495915,"user_tz":240,"elapsed":379,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"0bf2ab2f-75c3-4b5a-a5e1-37904e96c56c"},"source":["Y_train[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 9,  6,  8, 10, 11,  1,  6, 11,  2,  6], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"vNWcyExvKLXP"},"source":["### **seq2seq model**\n","We feed in the input sequence, which first goes through the encoder (an embedding layer followed by a single LSTM layer), which outputs a vector, then it goes through a decoder (a single LSTM layer, followed by a dense output layer), which outputs a sequence of vectors, each representing the estimated probabilities for all possible output character.\n","\n","Since the decoder expects a sequence as input, we repeat the vector (which is output by the decoder) as many times as the longest possible output sequence.\n","\n","\n","**LSTM** Long-Short-Term Memory, In keras you can use the LSTM layer, it will perform much better; training will coverge faster, and it will detect long-term dependencies in the data.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bfYWpEYQKeqb"},"source":["### **Build (layers)compile and train model with history**\n","    - embedding size: 32. epochs =20\n","    - encoder (embedding layer + single LSTM layer\n","    - decoder (single LSTM layer + dense output layer\n","    - keras LSTM layers: 128\n","    - dense layer activation softmax\n","    - optimizer Nadam. Loss Sparse categorical crossentropy\n"]},{"cell_type":"markdown","metadata":{"id":"SlRVwM1eN6xR"},"source":["**Embedding layer**: It is defined as the first hidden layer of a network. It must specify 3 arguments:\n","- **input_dim**: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n","- **output_dim**: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n","- **input_length**: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n"]},{"cell_type":"code","metadata":{"id":"8-feie0AMvZd"},"source":["# encoder (embedding layer + single LSTM layer)\n","embedding_size = 32\n","max_output_length = Y_train.shape[1]\n","\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","encoder = keras.models.Sequential([\n","    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1,\n","                           output_dim=embedding_size,\n","                           input_shape=[None]), keras.layers.LSTM(128)\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xSQtQAQRKYd"},"source":["# decoder (single LSTM layer + dense output layer\n","decoder = keras.models.Sequential([\n","    keras.layers.LSTM(128, return_sequences=True),\n","    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n","])\n","\n","model = keras.models.Sequential([\n","    encoder,\n","    keras.layers.RepeatVector(max_output_length),\n","    decoder\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDRDBDrGSSCF"},"source":["####**Splitting Dataset into Training, Validation and Test Sets**\n","\n","---\n","\n","\n","**Training dataset**\n","- The actual dataset that we use to train the model(**weights and biases**) in the case of the Neural Network\n","\n","**Validation dataset: Known as Development [Dev set]**\n","- The validation set is the sample data used to evaluate of a *model fit* of a given model, the dataset helps during the 'development' stage of the model \n","\n","**Test Dataset**\n","The Test dataset is used to evaluate the model. It is used once a model is completely trained(using the train and validation sets)"]},{"cell_type":"code","metadata":{"id":"4R7MWWk0RmSD","colab":{"base_uri":"https://localhost:8080/","height":765},"executionInfo":{"status":"ok","timestamp":1603453764440,"user_tz":240,"elapsed":257707,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"1b4ff70e-16e4-414a-b533-2bf7d33480a6"},"source":["# compile and train model with history\n","optimizer = keras.optimizers.Nadam()\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n","              metrics=[\"accuracy\"])\n","history = model.fit(X_train, Y_train, epochs=20,\n","                    validation_data=(X_valid, Y_valid))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","313/313 [==============================] - 13s 42ms/step - loss: 1.8188 - accuracy: 0.3491 - val_loss: 1.3606 - val_accuracy: 0.4976\n","Epoch 2/20\n","313/313 [==============================] - 12s 40ms/step - loss: 1.3776 - accuracy: 0.5052 - val_loss: 1.2579 - val_accuracy: 0.5555\n","Epoch 3/20\n","313/313 [==============================] - 12s 39ms/step - loss: 1.2514 - accuracy: 0.5603 - val_loss: 1.0266 - val_accuracy: 0.6378\n","Epoch 4/20\n","313/313 [==============================] - 12s 40ms/step - loss: 0.8830 - accuracy: 0.6795 - val_loss: 0.7703 - val_accuracy: 0.7128\n","Epoch 5/20\n","313/313 [==============================] - 12s 40ms/step - loss: 0.8608 - accuracy: 0.6914 - val_loss: 0.7561 - val_accuracy: 0.7208\n","Epoch 6/20\n","313/313 [==============================] - 12s 40ms/step - loss: 0.6057 - accuracy: 0.7664 - val_loss: 0.5127 - val_accuracy: 0.7957\n","Epoch 7/20\n","313/313 [==============================] - 12s 40ms/step - loss: 0.6050 - accuracy: 0.7764 - val_loss: 0.4381 - val_accuracy: 0.8255\n","Epoch 8/20\n","313/313 [==============================] - 12s 39ms/step - loss: 0.3631 - accuracy: 0.8576 - val_loss: 0.3004 - val_accuracy: 0.8877\n","Epoch 9/20\n","313/313 [==============================] - 12s 39ms/step - loss: 0.3454 - accuracy: 0.8867 - val_loss: 0.2291 - val_accuracy: 0.9226\n","Epoch 10/20\n","313/313 [==============================] - 12s 40ms/step - loss: 0.1677 - accuracy: 0.9511 - val_loss: 0.1407 - val_accuracy: 0.9614\n","Epoch 11/20\n","313/313 [==============================] - 12s 39ms/step - loss: 0.1093 - accuracy: 0.9737 - val_loss: 0.0857 - val_accuracy: 0.9818\n","Epoch 12/20\n","313/313 [==============================] - 13s 40ms/step - loss: 0.0694 - accuracy: 0.9865 - val_loss: 0.0539 - val_accuracy: 0.9908\n","Epoch 13/20\n","313/313 [==============================] - 13s 40ms/step - loss: 0.0375 - accuracy: 0.9956 - val_loss: 0.0321 - val_accuracy: 0.9963\n","Epoch 14/20\n","313/313 [==============================] - 12s 40ms/step - loss: 0.1375 - accuracy: 0.9717 - val_loss: 0.0311 - val_accuracy: 0.9969\n","Epoch 15/20\n","313/313 [==============================] - 13s 43ms/step - loss: 0.0214 - accuracy: 0.9984 - val_loss: 0.0190 - val_accuracy: 0.9987\n","Epoch 16/20\n","313/313 [==============================] - 13s 41ms/step - loss: 0.0135 - accuracy: 0.9994 - val_loss: 0.0132 - val_accuracy: 0.9992\n","Epoch 17/20\n","313/313 [==============================] - 13s 41ms/step - loss: 0.0093 - accuracy: 0.9998 - val_loss: 0.0096 - val_accuracy: 0.9995\n","Epoch 18/20\n","313/313 [==============================] - 13s 41ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 0.9998\n","Epoch 19/20\n","313/313 [==============================] - 13s 42ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 0.9999\n","Epoch 20/20\n","313/313 [==============================] - 13s 41ms/step - loss: 0.0292 - accuracy: 0.9949 - val_loss: 0.0062 - val_accuracy: 0.9997\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nWKCShG_S9NJ"},"source":["Looks great, we reach 100% validation accuracy! Let's use the model to make some predictions. We will need to be able to convert a sequence of character IDs to a readable string:\n","- create function (ids_to_date) \n","- X_new: use model to convert date (sept 17,2009 & Jul 14, 1789\n","- ids: iterate to show both dates"]},{"cell_type":"code","metadata":{"id":"JMZ-ujPATq5a"},"source":["# create function ids_to-date\n","def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n","    return [\"\".join([(\"?\" + chars)[index] for index in sequence])\n","            for sequence in ids]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rrv0ovOcT2u4"},"source":["# convert dates\n","X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qE2En-5hT_mg","colab":{"base_uri":"https://localhost:8080/","height":131},"executionInfo":{"status":"ok","timestamp":1603454126588,"user_tz":240,"elapsed":1093,"user":{"displayName":"Tenley Wiltshire","photoUrl":"https://lh4.googleusercontent.com/-_nPyKZbERyU/AAAAAAAAAAI/AAAAAAAAAfM/f_yjD2N5P_0/s64/photo.jpg","userId":"07699677317221241624"}},"outputId":"20d130e6-c195-4726-973a-f94b99325aca"},"source":["# iterate ids to product dates\n","ids = model.predict_classes(X_new)\n","for date_str in ids_to_date_strs(ids):\n","    print(date_str)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-21-a0f6961f4c8a>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n","Instructions for updating:\n","Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n","2009-09-17\n","1789-07-14\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bfMDli-aUIjH"},"source":["### **Summary**\n","The model worked, the Encoder–Decoder model converted a date string from one format: \"September 17, 2009\" to another format \"2009-09-17\""]}]}